{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5257627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6073e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: try to use torchaudio's resampler for speed/quality; else fallback to librosa\n",
    "try:\n",
    "    import torchaudio\n",
    "    TORCHAUDIO_AVAILABLE = True\n",
    "except Exception:\n",
    "    TORCHAUDIO_AVAILABLE = False\n",
    "    try:\n",
    "        import librosa\n",
    "        LIBROSA_AVAILABLE = True\n",
    "    except Exception:\n",
    "        LIBROSA_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c8e281b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Wav2Vec2GroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): Wav2Vec2FeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Wav2Vec2Encoder(\n",
       "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): Wav2Vec2SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load these once (do not reload for each file) for efficiency\n",
    "# choose a model appropriate for your needs; \"facebook/wav2vec2-base\" is a common base model\n",
    "MODEL_NAME = \"facebook/wav2vec2-base\"  # or \"facebook/wav2vec2-base-960h\", or a finetuned checkpoint\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2Model.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af33ec5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Wav2Vec2GroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): Wav2Vec2FeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Wav2Vec2Encoder(\n",
       "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): Wav2Vec2SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2Attention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put on GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "919b9063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resample_waveform(waveform: np.ndarray, orig_sr: int, target_sr: int) -> np.ndarray:\n",
    "    if orig_sr == target_sr:\n",
    "        return waveform\n",
    "    if TORCHAUDIO_AVAILABLE:\n",
    "        # waveform shape expected: (channels, time). Our waveform is 1D mono so expand dims\n",
    "        wav_t = torch.from_numpy(waveform).float().unsqueeze(0)  # (1, time)\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=target_sr)\n",
    "        wav_resampled = resampler(wav_t)\n",
    "        return wav_resampled.squeeze(0).numpy()\n",
    "    elif LIBROSA_AVAILABLE:\n",
    "        # librosa.resample expects float32\n",
    "        return librosa.resample(waveform.astype(\"float32\"), orig_sr, target_sr)\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"No resampler available: install torchaudio or librosa to enable resampling.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0063e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path: str, model_processor=processor, model_w2v=model, device=device) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract a fixed-size wav2vec2 embedding for an audio file.\n",
    "    Returns a 1D numpy array of size = model.hidden_size (e.g. 768).\n",
    "    \"\"\"\n",
    "    # 1) Load audio (soundfile reads sample rate and waveform)\n",
    "    audio, sr = sf.read(file_path)  # audio shape: (n_samples,) or (n_samples, channels)\n",
    "    # If stereo/multi-channel, take mean to convert to mono\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "\n",
    "    # 2) Resample if needed to the processor's sampling rate\n",
    "    target_sr = model_processor.feature_extractor.sampling_rate\n",
    "    if sr != target_sr:\n",
    "        audio = _resample_waveform(audio, sr, target_sr)\n",
    "        sr = target_sr\n",
    "\n",
    "    # 3) Preprocess: the processor expects list of arrays (batch)\n",
    "    inputs = model_processor(audio, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Move tensors to device\n",
    "    input_values = inputs.input_values.to(device)  # shape: (batch=1, seq_len)\n",
    "    attention_mask = inputs.attention_mask.to(device) if \"attention_mask\" in inputs else None\n",
    "    \n",
    "    # 4) Forward pass\n",
    "    with torch.no_grad():\n",
    "        # Request hidden states if you want; default returns last_hidden_state\n",
    "        outputs = model_w2v(input_values, attention_mask=attention_mask)\n",
    "        # last_hidden_state: (batch, seq_len, hidden_size)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    # 5) Mean-pool across time dimension to get a single vector (you can use other pooling: max, CLS-like)\n",
    "    # If attention_mask exists, compute masked mean to ignore padded positions\n",
    "    if attention_mask is not None:\n",
    "        mask = attention_mask.unsqueeze(-1)  # (batch, seq_len, 1)\n",
    "        masked_states = last_hidden_state * mask\n",
    "        summed = masked_states.sum(dim=1)       # sum over time -> (batch, hidden)\n",
    "        lengths = mask.sum(dim=1)               # number of valid frames -> (batch, 1)\n",
    "        mean_pooled = summed / lengths.clamp(min=1e-9)\n",
    "    else:\n",
    "        mean_pooled = last_hidden_state.mean(dim=1)  # (batch, hidden_size)\n",
    "\n",
    "    # Convert to numpy and return 1D array\n",
    "    embedding = mean_pooled.squeeze(0).cpu().numpy()\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc528b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m example_file = \u001b[33m\"\u001b[39m\u001b[33m/Users/shanoonissaka/Documents/school/thesis-project/datasets/audio/for-norm/testing/fake/file1_wav_16k_wav_norm_wav_mono_wav_silence.wav\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m emb = \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEmbedding shape:\u001b[39m\u001b[33m\"\u001b[39m, emb.shape) \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mextract_features\u001b[39m\u001b[34m(file_path, model_processor, model_w2v, device)\u001b[39m\n\u001b[32m     16\u001b[39m     sr = target_sr\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 3) Preprocess: the processor expects list of arrays (batch)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m inputs = \u001b[43mmodel_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Move tensors to device\u001b[39;00m\n\u001b[32m     22\u001b[39m input_values = inputs.input_values.to(device)  \u001b[38;5;66;03m# shape: (batch=1, seq_len)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/thesis-project/code/audio-detect/audio-detect/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:94\u001b[39m, in \u001b[36mWav2Vec2Processor.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou need to specify either an `audio` or `text` input to process.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m audio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     96\u001b[39m     encodings = \u001b[38;5;28mself\u001b[39m.tokenizer(text, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/thesis-project/code/audio-detect/audio-detect/lib/python3.11/site-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py:199\u001b[39m, in \u001b[36mWav2Vec2FeatureExtractor.__call__\u001b[39m\u001b[34m(self, raw_speech, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors, sampling_rate, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# convert into correct format for padding\u001b[39;00m\n\u001b[32m    197\u001b[39m encoded_inputs = BatchFeature({\u001b[33m\"\u001b[39m\u001b[33minput_values\u001b[39m\u001b[33m\"\u001b[39m: raw_speech})\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m padded_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoded_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# convert input values to correct format\u001b[39;00m\n\u001b[32m    209\u001b[39m input_values = padded_inputs[\u001b[33m\"\u001b[39m\u001b[33minput_values\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/thesis-project/code/audio-detect/audio-detect/lib/python3.11/site-packages/transformers/feature_extraction_sequence_utils.py:162\u001b[39m, in \u001b[36mSequenceFeatureExtractor.pad\u001b[39m\u001b[34m(self, processed_features, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors)\u001b[39m\n\u001b[32m    159\u001b[39m         first_element = required_input[index][\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_tf_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_element\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    163\u001b[39m         return_tensors = \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m is_torch_tensor(first_element):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/thesis-project/code/audio-detect/audio-detect/lib/python3.11/site-packages/transformers/utils/generic.py:199\u001b[39m, in \u001b[36mis_tf_tensor\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_tf_tensor\u001b[39m(x):\n\u001b[32m    196\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[33;03m    Tests if `x` is a tensorflow tensor or not. Safe to call even if tensorflow is not installed.\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_is_tensorflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/school/thesis-project/code/audio-detect/audio-detect/lib/python3.11/site-packages/transformers/utils/generic.py:192\u001b[39m, in \u001b[36m_is_tensorflow\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_is_tensorflow\u001b[39m(x):\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensor\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'tensorflow' has no attribute 'Tensor'"
     ]
    }
   ],
   "source": [
    "example_file = \"/Users/shanoonissaka/Documents/school/thesis-project/datasets/audio/for-norm/testing/fake/file1_wav_16k_wav_norm_wav_mono_wav_silence.wav\"\n",
    "emb = extract_features(example_file)\n",
    "print(\"Embedding shape:\", emb.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3211c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "  audio, sample_rate = librosa.load(file_path, sr=None)\n",
    "  mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
    "  return np.mean(mfccs.T, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f25f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real and fake audio samples (replace with your actual file paths)\n",
    "# Define the paths for real and fake audio files\n",
    "real_audio_path = '/Users/shanoonissaka/Documents/school/thesis-project/datasets/audio/for-norm/testing/real'\n",
    "fake_audio_path = '/Users/shanoonissaka/Documents/school/thesis-project/datasets/audio/for-norm/testing/fake'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a63c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all the .wav files from the directories\n",
    "real_audio_files = [os.path.join(real_audio_path, file) for file in os.listdir(real_audio_path) if file.endswith('.wav')]\n",
    "fake_audio_files = [os.path.join(fake_audio_path, file) for file in os.listdir(fake_audio_path) if file.endswith('.wav')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c95be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "for file in real_audio_files:\n",
    "  data.append(extract_features(file))\n",
    "  labels.append(0)  # Label 0 for real audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbcc2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in fake_audio_files:\n",
    "  data.append(extract_features(file))\n",
    "  labels.append(1)  # Label 1 for fake audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69c6b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "304b763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to CSV\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(X)\n",
    "# df['label'] = y\n",
    "# df.to_csv('/Users/shanoonissaka/Documents/school/thesis-project/code/audio-detect/data/features/audio_features_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d15d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8726a951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 28/29 [00:02<00:00,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1916, number of negative: 1791\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3315\n",
      "[LightGBM] [Info] Number of data points in the train set: 3707, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.516860 -> initscore=0.067466\n",
      "[LightGBM] [Info] Start training from score 0.067466\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:03<00:00,  8.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# use lazy prediction\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66c351fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "KNeighborsClassifier               1.00               1.00     1.00      1.00   \n",
      "SVC                                1.00               1.00     1.00      1.00   \n",
      "QuadraticDiscriminantAnalysis      1.00               1.00     1.00      1.00   \n",
      "ExtraTreesClassifier               1.00               1.00     1.00      1.00   \n",
      "AdaBoostClassifier                 1.00               1.00     1.00      1.00   \n",
      "SGDClassifier                      1.00               1.00     1.00      1.00   \n",
      "PassiveAggressiveClassifier        1.00               1.00     1.00      1.00   \n",
      "LogisticRegression                 1.00               1.00     1.00      1.00   \n",
      "LinearSVC                          1.00               1.00     1.00      1.00   \n",
      "CalibratedClassifierCV             1.00               1.00     1.00      1.00   \n",
      "Perceptron                         1.00               1.00     1.00      1.00   \n",
      "RandomForestClassifier             1.00               1.00     1.00      1.00   \n",
      "LGBMClassifier                     1.00               1.00     1.00      1.00   \n",
      "XGBClassifier                      1.00               1.00     1.00      1.00   \n",
      "NuSVC                              1.00               1.00     1.00      1.00   \n",
      "LinearDiscriminantAnalysis         1.00               1.00     1.00      1.00   \n",
      "RidgeClassifier                    1.00               1.00     1.00      1.00   \n",
      "RidgeClassifierCV                  1.00               1.00     1.00      1.00   \n",
      "BaggingClassifier                  0.99               0.99     0.99      0.99   \n",
      "GaussianNB                         0.99               0.99     0.99      0.99   \n",
      "NearestCentroid                    0.99               0.99     0.99      0.99   \n",
      "LabelPropagation                   0.99               0.99     0.99      0.99   \n",
      "LabelSpreading                     0.99               0.99     0.99      0.99   \n",
      "DecisionTreeClassifier             0.98               0.98     0.98      0.98   \n",
      "BernoulliNB                        0.97               0.97     0.97      0.97   \n",
      "ExtraTreeClassifier                0.96               0.96     0.96      0.96   \n",
      "DummyClassifier                    0.49               0.50     0.50      0.32   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "KNeighborsClassifier                 0.04  \n",
      "SVC                                  0.03  \n",
      "QuadraticDiscriminantAnalysis        0.02  \n",
      "ExtraTreesClassifier                 0.20  \n",
      "AdaBoostClassifier                   0.28  \n",
      "SGDClassifier                        0.01  \n",
      "PassiveAggressiveClassifier          0.02  \n",
      "LogisticRegression                   0.06  \n",
      "LinearSVC                            0.02  \n",
      "CalibratedClassifierCV               0.04  \n",
      "Perceptron                           0.02  \n",
      "RandomForestClassifier               0.57  \n",
      "LGBMClassifier                       0.42  \n",
      "XGBClassifier                        0.25  \n",
      "NuSVC                                0.32  \n",
      "LinearDiscriminantAnalysis           0.04  \n",
      "RidgeClassifier                      0.02  \n",
      "RidgeClassifierCV                    0.04  \n",
      "BaggingClassifier                    0.19  \n",
      "GaussianNB                           0.01  \n",
      "NearestCentroid                      0.05  \n",
      "LabelPropagation                     0.23  \n",
      "LabelSpreading                       0.39  \n",
      "DecisionTreeClassifier               0.05  \n",
      "BernoulliNB                          0.01  \n",
      "ExtraTreeClassifier                  0.02  \n",
      "DummyClassifier                      0.01  \n"
     ]
    }
   ],
   "source": [
    "print(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
